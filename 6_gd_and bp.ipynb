{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "\n", "# -------------------------\n", "# Activation functions\n", "# -------------------------\n", "def sigmoid(x):\n", "    return 1 / (1 + np.exp(-x))\n", "\n", "def sigmoid_derivative_from_output(sigmoid_out):\n", "    return sigmoid_out * (1 - sigmoid_out)\n", "\n", "def relu(x):\n", "    return np.maximum(0, x)\n", "\n", "def relu_derivative_from_preact(z):\n", "    return (z > 0).astype(float)\n", "\n", "# -------------------------\n", "# Loss (MSE) and derivative\n", "# -------------------------\n", "def mse_loss(y_true, y_pred):\n", "    return np.mean((y_true - y_pred) ** 2)\n", "\n", "def mse_loss_derivative(y_true, y_pred):\n", "    m = y_true.shape[0]\n", "    return (2.0 / m) * (y_pred - y_true)\n", "\n", "# -------------------------\n", "# Deep Neural Network\n", "# -------------------------\n", "class DeepNeuralNetwork:\n", "    def __init__(self, layer_sizes, learning_rate=0.1, seed=42):\n", "        np.random.seed(seed)\n", "        self.layer_sizes = layer_sizes\n", "        self.L = len(layer_sizes) - 1\n", "        self.lr = learning_rate\n", "        self.W, self.b = [], []\n", "        for i in range(self.L):\n", "            in_dim, out_dim = layer_sizes[i], layer_sizes[i + 1]\n", "            limit = np.sqrt(6.0 / (in_dim + out_dim))\n", "            self.W.append(np.random.uniform(-limit, limit, (in_dim, out_dim)))\n", "            self.b.append(np.zeros((1, out_dim)))\n", "\n", "    def forward(self, X):\n", "        activations, pre_acts = [X], []\n", "        for l in range(self.L):\n", "            z = activations[-1].dot(self.W[l]) + self.b[l]\n", "            pre_acts.append(z)\n", "            a = sigmoid(z) if l == self.L - 1 else relu(z)\n", "            activations.append(a)\n", "        self.activations, self.pre_acts = activations, pre_acts\n", "        return activations[-1]\n", "\n", "    def backward(self, X, y):\n", "        m, grads_W, grads_b = X.shape[0], [None]*self.L, [None]*self.L\n", "        dA = mse_loss_derivative(y, self.activations[-1])\n", "        for l in reversed(range(self.L)):\n", "            z, a, a_prev = self.pre_acts[l], self.activations[l+1], self.activations[l]\n", "            dZ = dA * (sigmoid_derivative_from_output(a) if l == self.L-1 else relu_derivative_from_preact(z))\n", "            grads_W[l] = a_prev.T.dot(dZ) / m\n", "            grads_b[l] = np.sum(dZ, axis=0, keepdims=True) / m\n", "            dA = dZ.dot(self.W[l].T)\n", "        for l in range(self.L):\n", "            self.W[l] -= self.lr * grads_W[l]\n", "            self.b[l] -= self.lr * grads_b[l]\n", "\n", "    def train(self, X, y, epochs=1000, print_every=100):\n", "        for epoch in range(1, epochs+1):\n", "            y_pred = self.forward(X)\n", "            loss = mse_loss(y, y_pred)\n", "            self.backward(X, y)\n", "            if epoch % print_every == 0 or epoch == 1:\n", "                print(f\"Epoch {epoch}/{epochs} - Loss: {loss:.6f}\")\n", "\n", "    def predict_proba(self, X):\n", "        return self.forward(X)\n", "\n", "    def predict(self, X, threshold=0.5):\n", "        return (self.predict_proba(X) >= threshold).astype(int)\n", "\n", "# -------------------------\n", "# Example: XOR problem\n", "# -------------------------\n", "X = np.array([[0,0],[0,1],[1,0],[1,1]], dtype=float)\n", "y = np.array([[0],[1],[1],[0]], dtype=float)\n", "\n", "net = DeepNeuralNetwork(layer_sizes=[2,4,1], learning_rate=0.1, seed=1)\n", "net.train(X, y, epochs=5000, print_every=500)\n", "\n", "print(\"\\nPredictions:\")\n", "preds_proba = net.predict_proba(X)\n", "preds = net.predict(X)\n", "for xi, p, lab, true in zip(X, preds_proba.flatten(), preds.flatten(), y.flatten()):\n", "    print(f\"{xi} -> {p:.4f} (label={lab}, true={int(true)})\")\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.x"}}, "nbformat": 4, "nbformat_minor": 5}
